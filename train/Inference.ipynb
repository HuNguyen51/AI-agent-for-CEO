{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    ")\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_map = {\"\": 0}\n",
    "model_name = \"NousResearch/Hermes-3-Llama-3.2-3B\"\n",
    "finetune_model = \"Hermes-3-Llama-3.2-3B-Finetuned\"\n",
    "\n",
    "model_path = \"models/finetune_model/\"\n",
    "tokenizer_path = \"models/tokenizer/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload and merge\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     low_cpu_mem_usage=True,\n",
    "#     return_dict=True,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=device_map,\n",
    "# )\n",
    "\n",
    "# model = PeftModel.from_pretrained(base_model, finetune_model)\n",
    "# model = model.merge_and_unload()\n",
    "\n",
    "# model.save_pretrained(\"models/finetune_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load peft model\n",
    "peft_model= AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run text generation pipeline with our next model\n",
    "prompt = \"triệu chứng buồn nôn của tôi là bị làm sao vậy bác sĩ\"\n",
    "pipe = pipeline(task=\"text-generation\", model=peft_model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\")\n",
    "print(result[0]['generated_text'])\n",
    "# OUTPUT: <|im_start|> user triệu chứng buồn nôn của tôi là bị làm sao vậy bác sĩ <|im_end|> assistant Chào mừng bạn đến với Chat Doctor. Tôi hiểu mối quan tâm của bạn. Tôi sẽ giải quyết vấn đề của bạn. Bác sĩ sẽ cho bạn thuốc giảm đau và thuốc chống nôn. Bác sĩ sẽ không cho bạn thuốc giảm đau. Bác sĩ sẽ cho bạn thuốc chống nôn. Bạn có thể uống thuốc chống nôn của mình, sau đó bạn sẽ có thể bắt đầu ăn một số thức ăn nhẹ như cơm và bánh mì. Bạn có thể bắt đầu ăn thức ăn nhẹ như rau, cà rốt, rau diếp, rau chân vịt và cà chua. Bạn cũng có thể bắt đầu ăn các loại thức ăn nhẹ khác như thịt, gà, cá và trứng. Bạn cũng có thể bắt đầu ăn bánh mì, bánh mì và bánh quy. Bạn cũng có thể bắt đầu ăn các loại thức ăn nhẹ khác như rau"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
